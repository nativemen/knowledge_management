\documentclass[oneside]{book}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{thmtools}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{float}
\usepackage{hyperref}
\usepackage{inputenc}
\usepackage[english]{babel}
\usepackage{framed}
\usepackage[dvipsnames]{xcolor}
\usepackage{environ}
\usepackage{tcolorbox}
\usepackage{todonotes}
\usepackage{booktabs}       % 专业表格线
\usepackage{multirow}       % 合并单元格
\usepackage{caption}        % 表格标题
\usepackage{makecell}
\usepackage{natbib}         % 推荐的参考文献宏包
\tcbuselibrary{theorems,skins,breakable}

\usetikzlibrary{calc,arrows.meta,bending}

\setstretch{1.2}
\geometry{
    textheight=9in,
    textwidth=7in,
    top=1in,
    headheight=12pt,
    headsep=25pt,
    footskip=30pt
}

% Variables
\def\notetitle{Spectral Clustering Algorithms}
\def\noteauthor{
    \textbf{Learner}\\
    {\LaTeX} by Xin Wang\\
}
\def\notedate{Notes}

% The theorem system and user-defined commands
\input{theorems.tex}
\input{commands.tex}

% ------------------------------------------------------------------------------

\begin{document}
\title{
    \textbf{
        \LARGE{\notetitle} \vspace*{10\baselineskip}
    }
}
\author{\noteauthor}
\date{\notedate}

\maketitle
\newpage

\tableofcontents  % 目录
\listoffigures    % 图目录
\listoftables     % 表目录
\newpage

% ------------------------------------------------------------------------------

\chapter{Graph theory}

\section{Basic terminologies}{
    \defn{Network or Graph}{
        A network (or graph) consists of a set of nodes (or vertices, actors) and a set of edges (or links, ties) that connect those nodes\citep{CXone_2024}.
    }

    \defn{Neighbor}{
        Node j is called a neighbor of node i if (and only if) node i is connected to node j.
    }

    \defn{Degree}{
        The number of edges connected to a node. Node $i$’s degree is often written as $\deg(i)$.
    }

    \defn{Walk}{
        A list of edges that are sequentially connected to form a continuous route on a network.
    }

    \defn{Trail}{
        A walk that doesn’t go through any edge more than once.
    }

    \defn{Path}{
        A walk that doesn’t go through any node (and therefore any edge, too) more than once.
    }

    \defn{Cycle}{
        A walk that starts and ends at the same node without going through any node more than once on its way.
    }

    \defn{Subgraph}{
        Part of the graph.
    }

    \defn{Connected graph}{
        A graph in which a path exists between any pair of nodes.
    }

    \defn{Connected component}{
        A subgraph of a graph that is connected within itself but not connected to the rest of the graph.
    }
}

\section{Graphs}{
    \defn{Complete graph}{
        A graph in which any pair of nodes are connected.
    }

    \defn{Regular graph}{
        A graph in which all nodes have the same degree.Every complete graph is regular.
    }

    \defn{Bipartite (n-partite) graph}{
        A graph whose nodes can be divided into two (or n) groups so that no edge connects nodes within each group.
    }

    \defn{Tree graph}{
        A graph in which there is no cycle. A graph made of multiple trees is called a forest graph. Every tree or forest graph is bipartite.
    }

    \defn{Planar graph}{
        A graph that can be graphically drawn in a two-dimensional plane with no edge crossings. Every tree or forest graph is planar.

    }
}

\section{Edges}{
    \defn{Undirected edge}{
        A symmetric connection between nodes. If node $i$ is connected to node $j$ by an undirected edge, then node $j$ also recognizes node $i$ as its neighbor. A graph made of undirected edges is called an undirected graph. The Adjacency matrix of an undirected graph is always symmetric.
    }

    \defn{Directed edge}{
        An asymmetric connection from one node to another. Even if node $i$ is connected to node $j$ by a directed edge, the connection isn’t necessarily reciprocated from node $j$ to node $i$. A graph made of directed edges is called a directed graph. The Adjacency matrix of a directed graph is generally asymmetric.
    }

    \defn{Unweighted edge}{
        An edge without any weight value associated to it. There are only two possibilities between a pair of nodes in a network with unweighted edges; whether there is an edge between them or not. The Adjacency matrix of such a network is made of only $0$’s and $1$’s.
    }

    \defn{Weighted edge}{
        An edge with a weight value associated to it. A weight is usually given by a non-negative real number, which may represent a connection strength or distance between nodes, depending on the nature of the system being modeled. The definition of the Adjacency matrix can be extended to contain those edge weight values for networks with weighted edges. The sum of the weights of edges connected to a node is often called the node strength, which corresponds to a node degree for unweighted graphs.
    }

    \defn{Multiple edges}{
        Edges that share the same origin and destination. Such multiple edges connect two nodes more than once.
    }

    \defn{Self-loop}{
        An edge that originates and ends at the same node.
    }

    \defn{Simple graph}{
        A graph that doesn’t contain directed, weighted, or multiple edges, or self-loops. Traditional graph theory mostly focuses on simple graphs.
    }

    \defn{Multigraph}{
        A graph that may contain multiple edges. Many mathematicians also allow multigraphs to contain self-loops. Multigraphs can be undirected or directed.
    }

    \defn{Hyperedge}{
        A generalized concept of an edge that can connect any number of nodes at once, not just two. A graph made of hyperedges is called a hypergraph (not covered in this textbook).
    }
}

\section{Matrices}

\subsection{Symmetric matrix}{
    \defn{Symmetric matrix}{
        A real matrix is called symmetric if $\mathbf{A}^{\mathbf{T}}=\mathbf{A}$.\\
        Symmetric matrices are also called selfadjoint.\\
        For complex matrices we would ask $\mathbf{A}^{*}=\overline{\mathbf{A}}^{\mathbf{T}}=\mathbf{A}$.
    }
    \label{Symmetric matrix}

    \cor{
        A symmetric matrix has real eigenvalues.
    }
    \pf{
        Extend the dot product to complex vectors by
        \[\langle\mathbf{v},\mathbf{w}\rangle=\sum_{i=1}^{n}\mathbf{v}_{i}\overline{\mathbf{w}_{i}}\], where $\overline{\mathbf{w}_{i}}$ is the complex conjugate of $\mathbf{w}_{i}$.\\
        For real vectors it is the usual dot product $\langle\mathbf{v},\mathbf{w}\rangle=\sum_{i=1}^{n}\mathbf{v}_{i}\mathbf{w}_{i}=\mathbf{v}\cdot\mathbf{w}$.\\
        For a real matrix $\mathbf{A}$, we have
        \begin{align}
            \langle\mathbf{A}\mathbf{v},\mathbf{w}\rangle
                &=\sum_{i=1}^{n}(\mathbf{A}\mathbf{v})_{i}\overline{w_{i}}\\
                &=\sum_{i=1}^{n}\left(\sum_{j=1}^{n}\mathbf{A}_{ij}v_{j}\right)\overline{w_{i}}\\
                &=\sum_{i=1}^{n}\sum_{j=1}^{n}\mathbf{A}_{ij}\mathbf{v}_{j}\overline{\mathbf{w}_{i}}
        \end{align}
        Since $\mathbf{A}$ is a real matrix, then $\mathbf{A}_{ij}$ is real, therefore $\overline{\mathbf{A}_{ij}}=\mathbf{A}_{ij}$.\\
        We can write the $j$-th component of the complex conjugate of the product of matrix $\mathbf{A}$ and vector $\mathbf{w}$ as:
        \[\overline{\left(\mathbf{A}\mathbf{w}\right)_{j}}=\sum_{i=1}^{n}\overline{\mathbf{A}_{ji}\mathbf{w}_{i}}\]
        When we compute the $j$-th component of the complex conjugate of the product of matrix $\mathbf{A}^{\mathbf{T}}$ and vector $\mathbf{w}$, we get:
        \[\overline{\left(\mathbf{A}^{\mathbf{T}}\mathbf{w}\right)_{j}}=\sum_{i=1}^{n}\overline{\left(\mathbf{A}^{\mathbf{T}}\right)_{ji}\mathbf{w}_{i}}\]
        Now, consider the transpose of matrix $\mathbf{A}$, denoted as $\mathbf{A}^{\mathbf{T}}$. The element $\left(\mathbf{A}^{\mathbf{T}}\right)_{ji}$ of the transpose matrix is equal to the element $\mathbf{A}_{ij}$ of the original matrix. Therefore, we have:
        \[\mathbf{A}_{ij}=\left(\mathbf{A}^{\mathbf{T}}\right)_{ji}\]
        then we obtain:
        \begin{align}
            \langle\mathbf{A}\mathbf{v},\mathbf{w}\rangle
                &=\sum_{i=1}^{n}\sum_{j=1}^{n}\mathbf{A}_{ij}\mathbf{v}_{j}\overline{\mathbf{w}_{i}}\\
                &=\sum_{i=1}^{n}\sum_{j=1}^{n}\overline{\mathbf{A}_{ij}}\mathbf{v}_{j}\overline{\mathbf{w}_{i}}\\
                &=\sum_{j=1}^{n}\mathbf{v}_{j}\left(\sum_{i=1}^{n}\overline{\mathbf{A}_{ij}}\overline{\mathbf{w}_{i}}\right)\\
                &=\sum_{j=1}^{n}\mathbf{v}_{j}\left(\sum_{i=1}^{n}\overline{\mathbf{A}_{ij}\mathbf{w}_{i}}\right)\\
                &=\sum_{j=1}^{n}\mathbf{v}_{j}\left(\sum_{i=1}^{n}\overline{(\mathbf{A}^{\mathbf{T}})_{ji}\mathbf{w}_{i}}\right)\\
                &=\sum_{j=1}^{n}\mathbf{v}_{j}\overline{\left(\mathbf{A}^{\mathbf{T}}\mathbf{w}\right)_{j}}\\
                &=\langle\mathbf{v},\mathbf{A}^{\mathbf{T}}\mathbf{w}\rangle
        \end{align}
        And assume that the symmetric matrix $\mathbf{A}$ has a complex eigenvalue $\lambda$, then
        \begin{align}
            \langle\lambda\mathbf{v},\mathbf{w}\rangle
                &=\sum_{i=1}^{n}\lambda\mathbf{v}_{i}\overline{\mathbf{w}_{i}}\\
                &=\lambda\sum_{i=1}^{n}\mathbf{v}_{i}\overline{\mathbf{w}_{i}}\\
                &=\lambda\langle\mathbf{v},\mathbf{w}\rangle
        \end{align}
        as well as
        \begin{align}
            \langle\mathbf{v},\lambda\mathbf{w}\rangle
                &=\sum_{i=1}^{n}\mathbf{v}_{i}\overline{\lambda\mathbf{w}_{i}}\\
                &=\sum_{i=1}^{n}\mathbf{v}_{i}\overline{\lambda}\overline{\mathbf{w}_{i}}\\
                &=\overline{\lambda}\sum_{i=1}^{n}\mathbf{v}_{i}\overline{\mathbf{w}_{i}}\\
                &=\overline{\lambda}\langle\mathbf{v},\mathbf{w}\rangle
        \end{align}
        Now, we have:
        \begin{align}
            \lambda\langle\mathbf{v},\mathbf{v}\rangle
                &=\langle\lambda\mathbf{v},\mathbf{v}\rangle\\
                &=\langle\mathbf{A}\mathbf{v},\mathbf{v}\rangle\\
                &=\langle\mathbf{v},\mathbf{A}^{\mathbf{T}}\mathbf{v}\rangle\\
                &=\langle\mathbf{v},\mathbf{A}\mathbf{v}\rangle\\
                &=\langle\mathbf{v},\lambda\mathbf{v}\rangle\\
                &=\overline{\lambda}\langle\mathbf{v},\mathbf{v}\rangle
        \end{align}
        Since $\mathbf{v}\neq0$, we have $\langle\mathbf{v},\mathbf{v}\rangle\neq0$. Thus $\lambda=\overline{\lambda}$, which means $\lambda \in \mathbb{R}$.
    }

    \cor{
        There is an orthogonal eigenbasis for a symmetric matrix $\mathbf{A}$ if all the eigenvalues of $\mathbf{A}$ are all different.
    }
    \pf{
        Assume that the symmetric matrix $\mathbf{A}$ has different real eigenvalues $\lambda$ and $\mu$ with corresponding eigenvectors $\mathbf{v}$ and $\mathbf{w}$, that means,
        \[\mathbf{A}\mathbf{v}=\lambda\mathbf{v}\ and\ \mathbf{A}\mathbf{w}=\mu\mathbf{w}\]
        The relation
        \begin{align}
            \lambda(\mathbf{v},\mathbf{w})
                &=(\lambda\mathbf{v},\mathbf{w})\\
                &=(\mathbf{A}\mathbf{v},\mathbf{w})\\
                &=(\mathbf{v},\mathbf{A}^{\mathbf{T}}\mathbf{w})\\
                &=(\mathbf{v},\mathbf{A}\mathbf{w})\\
                &=(\mathbf{v},\mu\mathbf{w})\\
                &=\overline{\mu}(\mathbf{v},\mathbf{w})\\
                &=\mu(\mathbf{v},\mathbf{w})
        \end{align}
        is only possible if $(\mathbf{v},\mathbf{w})=0$ if $\lambda\neq\mu$\\
        Therefore, the eigenvectors $\mathbf{v}$ and $\mathbf{w}$ are orthogonal.
    }

    \thm{Spectral theorem}{
        A symmetric matrix can be diagonalized with an orthonormal
        matrix S.
    }
    \pf{\todo{need more proof}}
}

\subsection{Standard basis}{
    \defn{Standard basis or natural basis or canonical basis}{
        In mathematics, the standard basis (also called natural basis or canonical basis) of a coordinate vector space (such as $\mathbb{R}^{n}$ or $\mathbb{C}^{n}$) is the set of vectors, each of whose components are all zero, except one that equals $1$. For example, in the case of the Euclidean plane
        an $n$-dimensional vector space$\mathbb{R}^{n}$, the standard basis vectors are a set of $n$ vectors, each of which has a $1$ in one coordinate and $0$ in all other coordinates. Mathematically, the standard basis vectors can be represented as:
        \[
            \mathbf{e}_{1}=
                \begin{bmatrix}
                    1\\
                    0\\
                    \vdots\\
                    0
                \end{bmatrix}
            ,
            \mathbf{e}_{2}=
                \begin{bmatrix}
                    0\\
                    1\\
                    \vdots\\
                    0
                \end{bmatrix}
            ,\cdots,
            \mathbf{e}_{n}=
            \begin{bmatrix}
                0\\
                0\\
                \vdots\\
                1
            \end{bmatrix}
        \]
        Here, $\mathbf{e}_{i}$ denotes the $i$-th standard basis vector. In an $n$-dimensional space, the set of standard basis vectors $\{\mathbf{e}_{1},\mathbf{e}_{2},\cdots,\mathbf{e}_{n}\}$ forms a basis for the space. This means that any vector $v$ in the space can be expressed as a linear combination of these standard basis vectors:
        \[\mathbf{v}=a_{1}\mathbf{e}_{1}+a_{2}\mathbf{e}_{2}+\cdots+a_{n}\mathbf{e}_{n}\]
        where $a_{1},a_{2},\cdots,a_{n}$ are the coordinates of the vector $\mathbf{v}$ with respect to the standard basis.
    }
    \label{Standard basis}
}

\subsection{Diagonal matrix}{
    \defn{Diagonal matrix}{
        A diagonal matrix is a matrix in which all off-diagonal entries are zero. That is, the matrix $\mathbf{D}=(d_{ij})$ with n columns and n rows is diagonal if
        \[\forall i,j \in {1,2,\cdots,n}, i \neq j \Longrightarrow d_{ij}=0\]
        However, the main diagonal entries are unrestricted.\\
        More often, however, diagonal matrix refers to square matrices, which can be specified explicitly as a square diagonal matrix. A square diagonal matrix is a symmetric matrix, so this can also be called a symmetric diagonal matrix. The main diagonal is the line of entries running from the top left to the bottom right of the matrix. Mathematically, a diagonal matrix $\mathbf{D}_{n \times n}$ can be represented as:
        \[
            \mathbf{D}_{n \times n}=
                \begin{bmatrix}
                    d_{1}  & 0     & \cdots & 0\\
                    0      & d_{2} & \cdots & 0\\
                    \vdots & 0     & \ddots & \vdots\\
                    0      & 0     & \cdots & d_{n}
                \end{bmatrix}
        \]
        where $d_{1},d_{2},\cdots,d_{n}$ are the diagonal elements of the matrix.
    }

    \cor{
        A diagonal matrix is symmetric.
        \[\mathbf{D}^{\mathbf{T}}=\mathbf{D}\]
    }

    \cor{
        A diagonal matrix $\mathbf{D}_{n \times n}=\operatorname{diag}(d_{1},\cdots,d_{n})$ is invertible if and only if all its diagonal elements $d_{1},\cdots,d_{n}$ are non-zero. The inverse of a diagonal matrix is also a diagonal matrix, with each diagonal element being the reciprocal of the original diagonal element.
        \[
            \mathbf{D}_{n \times n}^{-1}=
                \begin{bmatrix}
                    \frac{1}{d_{1}}  & 0               & \cdots & 0\\
                    0                & \frac{1}{d_{2}} & \cdots & 0\\
                    \vdots           & 0               & \ddots & \vdots\\
                    0                & 0               & \cdots & \frac{1}{d_{n}}
                \end{bmatrix}
        \]
        where $d_{i} \neq 0\ \ \forall i \in \{1,2,\cdots,n\}$
        \[\operatorname{diag}(d_{1},\cdots,d_{n})^{-1}=\operatorname{diag}(d_{1}^{-1},\cdots,d_{n}^{-1})\]
    }

    \cor{
        The eigenvalues of a diagonal matrix $\mathbf{D}_{n \times n}$ are its diagonal elements $d_{1},d_{2},\cdots,d_{n}$, and the eigenvectors are the standard basis vectors $\{\mathbf{e}_{1},\mathbf{e}_{2},\cdots,\mathbf{e}_{n}\}$ (\ref{Standard basis}).
    }

    \cor{
        The product of two diagonal matrices $\mathbf{D}_{n \times n}$ and $\mathbf{D'}_{n \times n}$ is another diagonal matrix, with diagonal elements being the product of the corresponding diagonal elements of the original matrices.
        \[
            \mathbf{D}_{n \times n}\mathbf{D'}_{n \times n}=
                \begin{bmatrix}
                    d_{1}  & 0      & \cdots & 0\\
                    0      & d_{2}  & \cdots & 0\\
                    \vdots & \vdots & \ddots & \vdots\\
                    0      & 0      & \cdots & d_{n,n}
                \end{bmatrix}
                \begin{bmatrix}
                    d'_{1} & 0      & \cdots & 0\\
                    0      & d'_{2} & \cdots & 0\\
                    \vdots & \vdots & \ddots & \vdots\\
                    0      & 0      & \cdots & d'_{n}
                \end{bmatrix}
                =
                \begin{bmatrix}
                    d_{1}d'_{1} & 0             & \cdots & 0\\
                    0           & d_{2}d'_{2,2} & \cdots & 0\\
                    \vdots      & \vdots        & \ddots & \vdots\\
                    0           & 0             & \cdots & d_{n}d'_{n}
                    \end{bmatrix}
        \]
        \[\operatorname{diag}(a_{1},\cdots,a_{n})\operatorname{diag}(b_{1},\cdots,b_{n})=\operatorname{diag}(a_{1}b_{1},\cdots,a_{n}b_{n})\]
    }

    \cor{
        For matrix multiplication:
        \[\operatorname{diag}(a_{1},\cdots,a_{n})+\operatorname{diag}(b_{1},\cdots,b_{n})=\operatorname{diag}(a_{1}+b_{1},\cdots,a_{n}+b_{n})\]
    }

    \cor{
        The trace of a diagonal matrix $\mathbf{D}_{n \times n}$ (the sum of its diagonal elements) is equal to the sum of its eigenvalues.
        \[\operatorname{Tr}(\mathbf{D}_{n \times n})=\sum_{i=1}^{n}d_{i}\]
    }

    \cor{
        The determinant of a diagonal matrix $\mathbf{D}_{n \times n}$ is the product of its diagonal elements.
        \[\det(\mathbf{D}_{n \times n})=\prod_{i=1}^{n}d_{i}\]
    }

    \cor{
        For a diagonal matrix $\mathbf{D}_{n \times n}$ with diagonal elements $d_{1},d_{2},\cdots,d_{n}$, the $k$-th power of $\mathbf{D}_{n \times n}$ is another diagonal matrix, which can be expressed as:
        \begin{itemize}
            \item When $k=0$, $\mathbf{D}_{n \times n}^{0}$ is the identity matrix $\mathbf{I}$, provided all diagonal elements $d_{i}$ are non-zero ($d_{i}\neq0$):
            \[
                \mathbf{D}_{n \times n}^{0}=
                    \mathbf{I}
                    =
                    \begin{bmatrix}
                        1      & 0 & \cdots & 0\\
                        0      & 1 & \cdots & 0\\
                        \vdots & 0 & \ddots & \vdots\\
                        0      & 0 & \cdots & 1
                    \end{bmatrix}
            \]
            \item For positive integers $k$, $\mathbf{D}_{n \times n}^{k}$ is computed by raising each diagonal element to the $k$-th power:
            \[
                \mathbf{D}_{n \times n}^{k}=
                    \begin{bmatrix}
                        d_{1}^{k}  & 0         & \cdots & 0\\
                        0          & d_{2}^{k} & \cdots & 0\\
                        \vdots     & 0         & \ddots & \vdots\\
                        0          & 0         & \cdots & d_{n}^{k}
                    \end{bmatrix}
            \]
            \item For negative integers $-k$, $\mathbf{D}_{n \times n}^{-k}$ is computed by taking the reciprocal of each diagonal element raised to the $k$-th power, assuming all diagonal elements $d_{i}$ are non-zero ($d_{i}\neq0$):
            \[
                \mathbf{D}_{n \times n}^{-k}=
                    \begin{bmatrix}
                        d_{1}^{-k} & 0          & \cdots & 0\\
                        0          & d_{2}^{-k} & \cdots & 0\\
                        \vdots     & 0          & \ddots & \vdots\\
                        0          & 0          & \cdots & d_{n}^{-k}
                    \end{bmatrix}
                    =
                    \begin{bmatrix}
                        \frac{1}{d_{1}^{k}} & 0                   & \cdots & 0\\
                        0                   & \frac{1}{d_{2}^{k}} & \cdots & 0\\
                        \vdots              & 0                   & \ddots & \vdots\\
                        0                   & 0                   & \cdots & \frac{1}{d_{n}^{k}}
                    \end{bmatrix}
            \]
            \item When $k$ is a fraction, the power of a diagonal matrix $\mathbf{D}_{n \times n}$ is still well-defined, provided that all the diagonal elements are non-negative ($d_{i}\geq0$ if $k$ is a real number) or positive ($d_{i}>0$ if $k$ is a complex number).\\
            For example, if $k=\frac{1}{2}$, then $\mathbf{D}_{n \times n}^{\frac{1}{2}}$ (the square root of
            $\mathbf{D}_{n \times n}$) is:
            \[
                \mathbf{D}_{n \times n}^{\frac{1}{2}}=
                \begin{bmatrix}
                    \sqrt{d_{1}} & 0            & \cdots & 0\\
                    0            & \sqrt{d_{2}} & \cdots & 0\\
                    \vdots       & 0            & \ddots & \vdots\\
                    0            & 0            & \cdots & \sqrt{d_{n}}
                \end{bmatrix}
            \]
            if if $k=-\frac{1}{2}$, then $\mathbf{D}_{n \times n}^{-\frac{1}{2}}$ is:
            \[
                \mathbf{D}_{n \times n}^{-\frac{1}{2}}=
                \begin{bmatrix}
                    \frac{1}{\sqrt{d_{1}}} & 0                      & \cdots & 0\\
                    0                      & \frac{1}{\sqrt{d_{2}}} & \cdots & 0\\
                    \vdots                 & 0                      & \ddots & \vdots\\
                    0                      & 0                      & \cdots & \frac{1}{\sqrt{d_{n}}}
                \end{bmatrix}
            \]
        \end{itemize}
    }
}

\subsection{Degree matrix}{
    \defn{Degree matrix}{
        Given a graph $\mathbf{G}=(\mathbf{V},\mathbf{E})$ with $|\mathbf{V}|=n$, $\mathbf{V}$ is the vertex set, and $\mathbf{E}$ is the edge set, the Degree matrix $\mathbf{D}_{n \times n}$ for $\mathbf{G}$ is a $n \times n$ diagonal matrix defined as
        \[
            \mathbf{D}_{ij}=
                \left\{
                    \begin{array}{ll}
                        \deg(v_{i}) & i=j \\
                        0           & i \neq j
                    \end{array}
                \right.
        \]
        where the degree $\deg(v_{i})$ of a vertex counts the number of times an edge terminates at that vertex.\\
        In an \textbf{undirected graph}, this means that each loop increases the degree of a vertex by two.\\
        In a \textbf{directed graph}, the term degree may refer either to indegree (the number of incoming edges at each vertex) or outdegree (the number of outgoing edges at each vertex)
    }
    \label{Degree matrix}

    \cor{
        Since the Degree matrix $\mathbf{D}_{n \times n}$ for $\mathbf{G}$ is a $n \times n$ diagonal matrix, it is also symmetric, meaning.
        \[\mathbf{D}_{ij}=\mathbf{D}_{ji}\]
    }
}

\subsection{Adjacency matrix}{
    \defn{Adjacency matrix}{
        Given a graph $\mathbf{G}=(\mathbf{V},\mathbf{E})$ with $|\mathbf{V}|=n$, $\mathbf{V}$ is the vertex set, and $\mathbf{E}$ is the edge set, the Adjacency matrix $\mathbf{A}_{n \times n}$ for $\mathbf{G}$ is a $n \times n$ matrix defined as
        \[
            \mathbf{A}_{ij}=
                \left\{
                    \begin{array}{ll}
                        1 & (v_{i},v_{j}) \in E \\
                        0 & (v_{i},v_{j}) \notin E
                    \end{array}
                \right.
        \]
        where $(v_i,v_j)$ is an edge in $\mathbf{E}$.\\
        A matrix with rows and columns labeled by nodes, whose $i$-th row, $j$-th column component  $a_{ij}$ is 1 if node i is a neighbor of node j, or 0 otherwise.
    }
    \label{Adjacency matrix}

    \cor{
        The diagonal elements of the Adjacency matrix are all zero, since edges from a vertex to itself (loops) are not allowed in simple graphs.
        \[\mathbf{A}_{ii}=0\]
    }

    \cor{
        For an undirected graph, the Adjacency matrix is symmetric, meaning.
        \[\mathbf{A}_{ij}=\mathbf{A}_{ji}\]
        For a directed graph, the Adjacency matrix may not be symmetric.
    }

    \defn{Adjacency list}{
        Adjacency list $\mathbf{A}$ list of lists of nodes whose $i$-th component is the list of node $i$’s neighbors.
    }
}

\subsection{Weighted Adjacency matrix}{
    \defn{Weighted Adjacency matrix}{
        Let $\mathbf{G}=(\mathbf{V},\mathbf{W})$ be an undirected graph, where V is the n vertex set, and $\mathbf{W}_{n \times n}$ is the $n \times n$ Weighted Adjacency matrix. Let $w_{ij}$ be the non-negative weight between nodes $i$, $j$.
        \[
            \mathbf{W}_{ij}=
                \left\{
                    \begin{array}{ll}
                        w_{ij} & (v_{i},v_{j}) \in E \\
                        0      & (v_{i},v_{j}) \notin E
                    \end{array}
                \right.
        \]
        An edge exists between nodes i and j if $w_{ij}>0$.\\
        For an unweighted graph, $w_{ij}=1$ if there is an edge; $w_{ij}=0$ otherwise.
    }
    \label{Weighted Adjacency matrix}

    \cor{
        For undirected graphs, the Weighted Adjacency matrix is symmetric, meaning
        \[\mathbf{W}_{ij}=\mathbf{W}_{ji}\]
        For a directed graph, the Weighted Adjacency matrix may not be symmetric.
    }

    \cor{
        For a simple graph, the diagonal elements of the Weighted Adjacency matrix are all 0, indicating no self-loops.
        \[\mathbf{W}_{ii}=0\ \ \forall i \in \{0,1,\cdots,n\}\]
    }

    \cor{
        The diagonal elements of the Degree Matrix $\mathbf{D}$ are the sums of the elements in each row of the Weighted Adjacency Matrix $\mathbf{W}$.\\
        Let the diagonal element $\mathbf{D}_{ii}$ represents the degree of vertex $i$, which is the sum of the weights of the edges connected to vertex $i$. Specifically:
        \[\mathbf{D}_{ii}=\sum_{j=0}^{n-1}\mathbf{W}_{ij}\]
        Therefore, the Degree Matrix $\mathbf{D}$ can be expressed as:
        \[\mathbf{D}=\operatorname{diag}\left(\sum_{j=0}^{n-1}\mathbf{W}_{ij}\right)\]
        where $\operatorname{diag}(\cdot)$ denotes constructing a diagonal matrix with the values inside the parentheses as its diagonal elements.
    }
}

\subsection{Weighted vertex-edge matrix}{
    \defn{Weighted vertex-edge matrix}{
        The edge between nodes $i$ and $j$ can be represented as an n-dimensional vector:
        \[\mathbf{e}=\sqrt{w_{ij}}\left[0,\cdots,0,1,0,\cdots,0,-1,0,\cdots,0\right]^{\mathbf{T}}\]
        where the $i$-th location is 1, the $j$-th location is -1, and other locations are 0.
        \[
            \mathbf{e}(\tau)=
                \left\{
                    \begin{array}{ll}
                        w_{ij}^{\frac{1}{2}}  & \text{if }\tau\text{ is }$i$,\\
                        -w_{ij}^{\frac{1}{2}} & \text{if }\tau\text{ is }$j$,\\
                        0                     & \text{otherwise}
                    \end{array}
                \right.
        \]
        Here, $\mathbf{e}(\tau)$ denotes the $\tau$-th value of the edge vector $\mathbf{e}$. For undirected graphs, the negative endpoint can be either at the $\tau=i$ location or at the $\tau=j$ location. Let $\mathbf{E}_{n \times m}=[\mathbf{e}_{1},\cdots,\mathbf{e}_{m}]$
        be the weighted vertex-edge matrix of size $n \times m$ obtained by taking the $m$ edge vectors as its columns. (The order of the edges is arbitrary.)
    }
    \label{Weighted vertex-edge matrix}
}

\subsection{Normalized weighted vertex-edge matrix}{
    \defn{Normalized weighted vertex-edge matrix}{
        Let $\mathbf{D}$ be the Degree matrix of the graph with the $i$-th diagonal element:
        \[d_{i}=\sum_{j=1}^{n}w_{ij}\]
        Let $\mathcal{E}_{n \times m}=[\epsilon_{1},\cdots,\epsilon_{m}]$ be the Normalized weighted vertex-edge matrix with each $n$-dimensional edge $\epsilon$ vector normalized:
        \[\mathcal{E}_{n \times m}=\mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{E}_{n \times m}\]
        with $\epsilon=\sqrt{w_{ij}}\left[0,\cdots,0,\frac{1}{\sqrt{d_{i}}},0,\cdots,0,-\frac{1}{\sqrt{d_{j}}},0,\cdots,0\right]^{\mathbf{T}}$
        where the $i$-th location is $\sqrt{d_{i}}$, the $j$-th location is $-\sqrt{d_{j}}$, and other locations are 0.
        \[
            \epsilon(\tau)=
                \left\{
                    \begin{array}{ll}
                        d_{i}^{-\frac{1}{2}}w_{ij}^{\frac{1}{2}}  & \text{if }\tau\text{ is }$i$,\\
                        -d_{j}^{-\frac{1}{2}}w_{ij}^{\frac{1}{2}} & \text{if }\tau\text{ is }$j$,\\
                        0                                        & \text{otherwise}
                    \end{array}
                \right.
        \]
        Here, $\epsilon(\tau)$ denotes the $\tau$-th value of the edge vector $\epsilon$.
        \[\epsilon(\tau)=\mathbf{D}^{-\frac{1}{2}}\mathbf{e}(\tau)\]
    }
    \label{Normalized weighted vertex-edge matrix}
    \pf{
        \begin{align}
            \epsilon(\tau)
                &=\mathbf{D}^{-\frac{1}{2}}\mathbf{e}(\tau)\\
                &=
                \begin{bmatrix}
                    d_{1}^{-\frac{1}{2}} & 0                    & \cdots & 0\\
                    0                    & d_{2}^{-\frac{1}{2}} & \cdots & 0\\
                    \vdots               & \vdots               & \ddots & \vdots\\
                    0                    & \cdots               & \cdots & d_{n}^{-\frac{1}{2}}
                \end{bmatrix}
                \sqrt{w_{ij}}
                \begin{bmatrix}
                    0\\
                    \cdots\\
                    0\\
                    1\\
                    0\\
                    \cdots\\
                    0\\
                    -1\\
                    0\\
                    \cdots\\
                    0
                \end{bmatrix}\\
                &=\sqrt{w_{ij}}
                \begin{bmatrix}
                    0\\
                    \cdots\\
                    0\\
                    \frac{1}{\sqrt{d_{i}}}\\
                    0\\
                    \cdots\\
                    0\\
                    -\frac{1}{\sqrt{d_{j}}}\\
                    0\\
                    \cdots\\
                    0
                \end{bmatrix}\\
                &=\sqrt{w_{ij}}\left[0,\cdots,0,\frac{1}{\sqrt{d_{i}}},0,\cdots,0,-\frac{1}{\sqrt{d_{j}}},0,\cdots,0\right]^{\mathbf{T}}
        \end{align}
    }
}

\subsection{Laplacian matrix}{
    \defn{Laplacian matrix}{
        Given a simple $\mathbf{G}$ with $n$ vertices $v_{1},\cdots,v_{n}$, its Laplacian matrix $\mathbf{L}_{n \times n}$ is defined element-wise as
        \[
            \mathbf{L}_{ij}=
                \left\{
                    \begin{array}{ll}
                        \deg(v_i) & i=j \\
                        -w_{ij}   & i \neq j \text{ and } (v_{i},v_{j}) \in E \\
                        0         & \text{otherwise}
                    \end{array}
                \right.
        \]
        or equivalently by the matrix
        \[\mathbf{L}_{n \times n}=\mathbf{D}_{n \times n}-\mathbf{W}_{n \times n}\]
        where $\mathbf{D}_{n \times n}$ is the Degree matrix and $\mathbf{W}_{n \times n}$ is the Weighted Adjacency matrix of the graph $\mathbf{G}$.\\
        For an unweighted graph, $w_{ij}=1$ if there is an edge. then
        \[
            \mathbf{L}_{ij}=
                \left\{
                    \begin{array}{ll}
                        \deg(v_i) & i=j \\
                        -1        & i \neq j \text{ and } (v_{i},v_{j}) \in E \\
                        0         & \text{otherwise}
                    \end{array}
                \right.
        \]
        or equivalently by the matrix
        \[\mathbf{L}_{n \times n}=\mathbf{D}_{n \times n}-\mathbf{A}_{n \times n}\]
        where $\mathbf{D}_{n \times n}$ is the Degree matrix and $\mathbf{A}_{n \times n}$ is the Adjacency matrix of the graph $\mathbf{G}$.\\
        Since $\mathbf{G}$ is a simple graph, $\mathbf{A}$ only contains $1$s or $0$s and its diagonal elements are all 0s.
    }
    \label{Laplacian matrix}

    \cor{
        For an undirected graph, the Laplacian matrix $\mathbf{L}_{n \times n}$ is a symmetric matrix.
    }
    \pf{
        The Laplacian matrix $\mathbf{L}_{n \times n}$ is defined as $\mathbf{L}_{n \times n} = \mathbf{D}_{n \times n}-\mathbf{W}_{n \times n}$, where $\mathbf{D}_{n \times n}$ is the degree matrix and $\mathbf{W}_{n \times n}$ is the weighted adjacency matrix.\\
        For an undirected graph, the adjacency matrix $\mathbf{W}_{n \times n}$ is symmetric, meaning $\mathbf{W}_{n \times n}=\mathbf{W}_{n \times n}^T$.\\
        The degree matrix $\mathbf{D}_{n \times n}$ is diagonal and therefore symmetric.\\
        Since both $\mathbf{D}_{n \times n}$ and $\mathbf{W}_{n \times n}$ are symmetric, their difference $\mathbf{L}_{n \times n}=\mathbf{D}_{n \times n}-\mathbf{W}_{n \times n}$ is also symmetric.
    }

    \cor{
        For an undirected graph, the eigenvalues of the Laplacian matrix $\mathbf{L}$ are real and non-negative.
    }
    \pf{
        Since the Laplacian matrix $\mathbf{L}$ is symmetric, then its eigenvalues are real (\ref{Symmetric matrix}).
    }

    \cor{
        For the undirected graph that both the Adjacency matrix $\mathbf{A}$ and the Laplacian matrix $\mathbf{L}$ are symmetric, and that row- and column-sums of the Laplacian matrix are all zeros (which directly implies that the Laplacian matrix is singular).
    }

    \cor{
        Let $\mathbf{E}_{n \times m}=[\mathbf{e}_{1},\cdots,\mathbf{e}_{m}]$ be the $n \times m$ weighted vertex-edge matrix in the an undirected graph of $n$ vertices and $m$ edges (\ref{Weighted vertex-edge matrix}).\\
        The graph Laplacian matrix can be computed as follows
        \[\mathbf{L}_{n \times n}=\mathbf{E}_{n \times m}\mathbf{E}_{n \times m}^{\mathbf{T}}=\sum_{t=1}^{m}\mathbf{e}_{t}\mathbf{e}_{t}^{\mathbf{T}}\]
    }
    \pf{
        Since $\mathbf{E}_{n \times m}=[\mathbf{e}_{1},\cdots,\mathbf{e}_{m}]$ be the $n \times m$ weighted vertex-edge matrix in the an undirected graph of $n$ vertices and $m$ edges.\\
        Let $\mathbf{e}(\tau)$ denotes the $\tau$-th value of the edge vector $\mathbf{e}$. For undirected graphs, the negative endpoint can be either at the $\tau=i$ location or at the $\tau=j$ location.
        \[
            \mathbf{e}(\tau)=
                \left\{
                    \begin{array}{ll}
                        w_{ij}^{\frac{1}{2}}  & \text{if }\tau\text{ is }$i$,\\
                        -w_{ij}^{\frac{1}{2}} & \text{if }\tau\text{ is }$j$,\\
                        0                     & \text{otherwise}
                    \end{array}
                \right.
        \]
        Then
        \begin{align}
            \mathbf{E}_{n \times m}\mathbf{E}_{n \times m}^{\mathbf{T}}
                &=
                \begin{bmatrix}
                    \mathbf{e}_{1} & \cdots & \mathbf{e}_{m}
                \end{bmatrix}
                \begin{bmatrix}
                    \mathbf{e}_{1}^{\mathbf{T}}\\
                    \vdots\\
                    \mathbf{e}_{m}^{\mathbf{T}}
                \end{bmatrix}\\
                &=\sum_{t=1}^{m}\mathbf{e}_{t}\mathbf{e}_{t}^{\mathbf{T}}\\
                &=
                \begin{bmatrix}
                    0      & \cdots & 0       & \cdots & 0       & \cdots & 0 \\
                    \vdots & \ddots & \vdots  & \ddots & \vdots  & \ddots & \vdots \\
                    0      & \cdots & w_{ij}  & \cdots & -w_{ij} & \cdots & 0 \\
                    \vdots & \ddots & \vdots  & \ddots & \vdots  & \ddots & \vdots \\
                    0      & \cdots & -w_{ij} & \cdots & w_{ij}  & \cdots & 0 \\
                    \vdots & \ddots & \vdots  & \ddots & \vdots  & \ddots & \vdots \\
                    0      & \cdots & 0       & \cdots & 0       & \cdots & 0 \\
                \end{bmatrix}
            \end{align}
        For each $\mathbf{e}_{t}$, its non-zero elements are at positions $i$ and $j$, with values $\sqrt{w_{ij}}$ and $-\sqrt{w_{ij}}$ respectively. Therefore,$\mathbf{e}_{t}\mathbf{e}_{t}^{\mathbf{T}}$ is an $n \times n$ matrix with $w_{ij}$ at positions $(i,i)$ and $(j,j)$, $-w_{ij}$ at positions $(i,j)$ and $(j,i)$, and 0 elsewhere.\\
        Summing all $\mathbf{e}_{t}\mathbf{e}_{t}^{\mathbf{T}}$ matrices results in a matrix $\mathbf{E}_{n \times m}\mathbf{E}_{n \times m}^{\mathbf{T}}$ where the diagonal elements are the degrees of the vertices (i.e., the degree matrix $\mathbf{D}_{n \times n}$), and the off-diagonal elements are the negative weights of the edges (i.e., the negative of the weighted adjacency matrix $\mathbf{W}_{n \times n}$).\\
        Specifically, for the $i$-th row and $i$-th column element of the matrix $\mathbf{E}_{n \times m}\mathbf{E}_{n \times m}^{\mathbf{T}}$:
        \[(\mathbf{E}_{n \times m}\mathbf{E}_{n \times m}^{\mathbf{T}})_{ii}=\sum_{t=1}^{m}(\mathbf{e}_{t}\mathbf{e}_{t}^{\mathbf{T}})_{ii}=\sum_{j \neq i}w_{ij}=d_{i}\]
        where $d_{i}$ is the degree of vertex i (\ref{Degree matrix}).\\
        For the $i$-th row and $j$-th column element of the matrix $\mathbf{E}_{n \times m}\mathbf{E}_{n \times m}^{\mathbf{T}}$ (where $i \neq j$):
        \[(\mathbf{E}_{n \times m}\mathbf{E}_{n \times m}^{\mathbf{T}})_{ij}=\sum_{t=1}^{m}(\mathbf{e}_{t}\mathbf{e}_{t}^{\mathbf{T}})_{ij}=-w_{ij}\]
        where $w_{ij}$ is the weight of the edge between vertices $i$ and $j$ (\ref{Weighted Adjacency matrix}).\\
        Thus, $\mathbf{E}_{n \times m}\mathbf{E}_{n \times m}^{\mathbf{T}}=\mathbf{D}_{n \times n}-\mathbf{W}_{n \times n}=\mathbf{L}_{n \times n}$.
    }
}

\subsection{Normalized Laplacian matrix}{
    \defn{Symmetrically normalized Laplacian matrix}{
        The symmetrically normalized Laplacian matrix can be computed as:
        \[\mathcal{L}_{sym}=\mathbf{D}^{-\frac{1}{2}}\mathbf{L}\mathbf{D}^{-\frac{1}{2}}=\mathbf{I}-\mathbf{D}^{-\frac{1}{2}}\mathbf{W}\mathbf{D}^{-\frac{1}{2}}\]
        The elements of $\mathcal{L}_{sym}$ are thus given by
        \[
            (\mathcal{L}_{sym})_{ij}=
                \left\{
                    \begin{array}{ll}
                        1                                        & if\ i=j\ and\ \deg(v_{i}) \neq 0\\
                        -\frac{w_{ij}}{\sqrt{\deg(v_{i})\deg(v_{j})}} & if\ i \neq j\ and\ v_{i}\ is\ adjacent\ to\ v_{j}\\
                        0                                        & otherwise
                    \end{array}
                \right.
        \]
    }
    \label{Symmetrically normalized Laplacian matrix}
    \pf{
        The Degree matrix $\mathbf{D}_{n \times n}$ is a diagonal matrix, and when $d_{i} \neq 0\ \ \forall i \in \{1,2,\cdots,n\}$, then
        \begin{align}
            \mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{D}_{n \times n}\mathbf{D}_{n \times n}^{-\frac{1}{2}}
                    &=
                    \begin{bmatrix}
                        \frac{1}{\sqrt{d_{1}}} & 0                      & \cdots & 0\\
                        0                      & \frac{1}{\sqrt{d_{2}}} & \cdots & 0\\
                        \vdots                 & \vdots                 & \ddots & \vdots\\
                        0                      & 0                      & \cdots & \frac{1}{\sqrt{d_{n}}}
                    \end{bmatrix}
                    \begin{bmatrix}
                        d_{1}  & 0      & \cdots & 0\\
                        0      & d_{2}  & \cdots & 0\\
                        \vdots & \vdots & \ddots & \vdots\\
                        0      & 0      & \cdots & d_{n}
                    \end{bmatrix}
                    \begin{bmatrix}
                        \frac{1}{\sqrt{d_{1}}} & 0                      & \cdots & 0\\
                        0                      & \frac{1}{\sqrt{d_{2}}} & \cdots & 0\\
                        \vdots                 & \vdots                 & \ddots & \vdots\\
                        0                      & 0                      & \cdots & \frac{1}{\sqrt{d_{n}}}
                    \end{bmatrix}\\
                    &=
                    \begin{bmatrix}
                        \frac{d_{1}}{\sqrt{d_{1}}\sqrt{d_{1}}} & 0                                    & \cdots & 0\\
                        0                                      & \frac{d_{2}}{\sqrt{d_{2}}\sqrt{d_{2}}} & \cdots & 0\\
                        \vdots                                 & \vdots                               & \ddots & \vdots\\
                        0                                      & 0                                    & \cdots & \frac{d_{n}}{\sqrt{d_{n}}\sqrt{d_{n}}}
                    \end{bmatrix}\\
                    &=
                    \begin{bmatrix}
                        1      & 0      & \cdots & 0\\
                        0      & 1      & \cdots & 0\\
                        \vdots & \vdots & \ddots & \vdots\\
                        0      & 0      & \cdots & 1
                    \end{bmatrix}\\
                    &=
                    \mathbf{I}_{n \times n}
        \end{align}
        Since $\mathbf{L}_{n \times n}=\mathbf{D}_{n \times n}-\mathbf{W}_{n \times n}$, then
        \begin{align}
            \mathcal{L}_{sym}
                &=\mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{L}_{n \times n}\mathbf{D}_{n \times n}^{-\frac{1}{2}}\\
                &=\mathbf{D}_{n \times n}^{-\frac{1}{2}}(\mathbf{D}_{n \times n}-\mathbf{W}_{n \times n})\mathbf{D}_{n \times n}^{-\frac{1}{2}}\\
                &=\mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{D}_{n \times n}\mathbf{D}_{n \times n}^{-\frac{1}{2}}-\mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{W}_{n \times n}\mathbf{D}_{n \times n}^{-\frac{1}{2}}\\
                &=\mathbf{I}_{n \times n}-\mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{W}_{n \times n}\mathbf{D}_{n \times n}^{-\frac{1}{2}}
        \end{align}
        And we have
        \begin{align}
            \mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{W}_{n \times n}\mathbf{D}_{n \times n}^{-\frac{1}{2}}
                &=
                \begin{bmatrix}
                    \frac{1}{\sqrt{d_{1}}} & 0                      & \cdots & 0\\
                    0                      & \frac{1}{\sqrt{d_{2}}} & \cdots & 0\\
                    \vdots                 & \vdots                 & \ddots & \vdots\\
                    0                      & 0                      & \cdots & \frac{1}{\sqrt{d_{n}}}
                \end{bmatrix}
                \begin{bmatrix}
                    w_{11} & w_{12} & \cdots & w_{1n}\\
                    w_{21} & w_{22} & \cdots & w_{2n}\\
                    \vdots & \vdots & \ddots & \vdots\\
                    w_{n1} & w_{n2} & \cdots & w_{nn}
                \end{bmatrix}
                \begin{bmatrix}
                    \frac{1}{\sqrt{d_{1}}} & 0                      & \cdots & 0\\
                    0                      & \frac{1}{\sqrt{d_{2}}} & \cdots & 0\\
                    \vdots                 & \vdots                 & \ddots & \vdots\\
                    0                      & 0                      & \cdots & \frac{1}{\sqrt{d_{n}}}
                \end{bmatrix}\\
                &=
                \begin{bmatrix}
                    \frac{w_{11}}{\sqrt{d_{1}}} & \frac{w_{12}}{\sqrt{d_{1}}} & \cdots & \frac{w_{1n}}{\sqrt{d_{1}}}\\
                    \frac{w_{21}}{\sqrt{d_{2}}} & \frac{w_{22}}{\sqrt{d_{2}}} & \cdots & \frac{w_{2n}}{\sqrt{d_{2}}}\\
                    \vdots                      & \vdots                      & \ddots & \vdots\\
                    \frac{w_{n1}}{\sqrt{d_{n}}} & \frac{w_{n2}}{\sqrt{d_{n}}} & \cdots & \frac{w_{nn}}{\sqrt{d_{n}}}
                \end{bmatrix}
                \begin{bmatrix}
                    \frac{1}{\sqrt{d_{1}}} & 0                      & \cdots & 0\\
                    0                      & \frac{1}{\sqrt{d_{2}}} & \cdots & 0\\
                    \vdots                 & \vdots                 & \ddots & \vdots\\
                    0                      & 0                      & \cdots & \frac{1}{\sqrt{d_{n}}}
                \end{bmatrix}\\
                &=
                \begin{bmatrix}
                    \frac{w_{11}}{\sqrt{d_{1}}\sqrt{d_{1}}} & \frac{w_{12}}{\sqrt{d_{1}}\sqrt{d_{2}}} & \cdots & \frac{w_{1n}}{\sqrt{d_{1}}\sqrt{d_{n}}}\\
                    \frac{w_{21}}{\sqrt{d_{2}}\sqrt{d_{1}}} & \frac{w_{22}}{\sqrt{d_{2}}\sqrt{d_{2}}} & \cdots & \frac{w_{2n}}{\sqrt{d_{2}}\sqrt{d_{n}}}\\
                    \vdots                                  & \vdots                                  & \ddots & \vdots\\
                    \frac{w_{n1}}{\sqrt{d_{n}}\sqrt{d_{1}}} & \frac{w_{n2}}{\sqrt{d_{n}}\sqrt{d_{2}}} & \cdots & \frac{w_{nn}}{\sqrt{d_{n}}\sqrt{d_{n}}}
                \end{bmatrix}
        \end{align}
        We know that the diagonal elements of the Weighted Adjacency matrix are all zero, that means $w_{ii}=0\ \ \forall i \in \{0,1,\cdots,n\}$ (\ref{Weighted Adjacency matrix}). And the $w_{ij}=0$ if $i \neq j$ and vertex $i$ is not adjacent to vertex $j$, then
        \[
            \mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{W}_{n \times n}\mathbf{D}_{n \times n}^{-\frac{1}{2}}=
                \begin{bmatrix}
                    0                                       & \frac{w_{12}}{\sqrt{d_{1}}\sqrt{d_{2}}} & \cdots & \frac{w_{1n}}{\sqrt{d_{1}}\sqrt{d_{n}}}\\
                    \frac{w_{21}}{\sqrt{d_{2}}\sqrt{d_{1}}} & 0                                       & \cdots & \frac{w_{2n}}{\sqrt{d_{2}}\sqrt{d_{n}}}\\
                    \vdots                                  & \vdots                                  & \ddots & \vdots\\
                    \frac{w_{n1}}{\sqrt{d_{n}}\sqrt{d_{1}}} & \frac{w_{n2}}{\sqrt{d_{n}}\sqrt{d_{2}}} & \cdots & 0
                \end{bmatrix}
        \]
        Thus, we have
        \begin{align}
            \mathcal{L}_{sym}
                &=\mathbf{I}_{n \times n}-\mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{W}_{n \times n}\mathbf{D}_{n \times n}^{-\frac{1}{2}}\\
                &=
                \begin{bmatrix}
                    1      & 0      & \cdots & 0\\
                    0      & 1      & \cdots & 0\\
                    \vdots & \vdots & \ddots & \vdots\\
                    0      & 0      & \cdots & 1
                \end{bmatrix}
                -
                \begin{bmatrix}
                    0                                       & \frac{w_{12}}{\sqrt{d_{1}}\sqrt{d_{2}}} & \cdots & \frac{w_{1n}}{\sqrt{d_{1}}\sqrt{d_{n}}}\\
                    \frac{w_{21}}{\sqrt{d_{2}}\sqrt{d_{1}}} & 0                                       & \cdots & \frac{w_{2n}}{\sqrt{d_{2}}\sqrt{d_{n}}}\\
                    \vdots                                  & \vdots                                  & \ddots & \vdots\\
                    \frac{w_{n1}}{\sqrt{d_{n}}\sqrt{d_{1}}} & \frac{w_{n2}}{\sqrt{d_{n}}\sqrt{d_{2}}} & \cdots & 0
                \end{bmatrix}\\
                &=
                \begin{bmatrix}
                    1                                        & -\frac{w_{12}}{\sqrt{d_{1}}\sqrt{d_{2}}} & \cdots & -\frac{w_{1n}}{\sqrt{d_{1}}\sqrt{d_{n}}}\\
                    -\frac{w_{21}}{\sqrt{d_{2}}\sqrt{d_{1}}} & 1                                        & \cdots & -\frac{w_{2n}}{\sqrt{d_{2}}\sqrt{d_{n}}}\\
                    \vdots                                   & \vdots                                   & \ddots & \vdots\\
                    -\frac{w_{n1}}{\sqrt{d_{n}}\sqrt{d_{1}}} & -\frac{w_{n2}}{\sqrt{d_{n}}\sqrt{d_{2}}} & \cdots & 1
                \end{bmatrix}
        \end{align}
        We know that the degree of the vertex $i$ is $d_{i}=\deg(v_{i})$, then the elements of $\mathcal{L}_{sym}$ are
        \[
            (\mathcal{L}_{sym})_{ij}=
                \left\{
                    \begin{array}{ll}
                        1                                        & if\ i=j\ and\ \deg(v_{i}) \neq 0\\
                        -\frac{w_{ij}}{\sqrt{\deg(v_{i})\deg(v_{j})}} & if\ i \neq j\ and\ v_{i}\ is\ adjacent\ to\ v_{j}\\
                        0                                        & otherwise
                    \end{array}
                \right.
        \]
    }

    \cor{
        \[\mathcal{L}_{sym}=\mathcal{E}\mathcal{E}^{\mathbf{T}}=\sum_{t=1}^{m}\mathbf{\epsilon}_{t}\mathbf{\epsilon}_{t}^{\mathbf{T}}\]
    }
    \pf{
        Since $\mathbf{L}_{n \times n}=\mathbf{E}_{n \times m}\mathbf{E}_{n \times m}^{\mathbf{T}}=\sum_{t=1}^{m}\mathbf{e}_{t}\mathbf{e}_{t}^{\mathbf{T}}$, $\mathcal{E}_{n \times m}=\mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{E}_{n \times m}$ and $\mathbf{D}_{n \times n}^{\mathbf{T}}=\mathbf{D}_{n \times n}$, then
        \begin{align}
            \mathcal{L}_{sym}
                &=\mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{L}_{n \times n}\mathbf{D}_{n \times n}^{-\frac{1}{2}}\\
                &=\mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{E}_{n \times m}\mathbf{E}_{n \times m}^{\mathbf{T}}(\mathbf{D}_{n \times n}^{\mathbf{T}})^{-\frac{1}{2}}\\
                &=\mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{E}_{n \times m}\mathbf{E}_{n \times m}^{\mathbf{T}}(\mathbf{D}_{n \times n}^{-\frac{1}{2}})^{\mathbf{T}}\\
                &=(\mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{E}_{n \times m})(\mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{E}_{n \times m})^{\mathbf{T}}\\
                &=\mathcal{E}\mathcal{E}^{\mathbf{T}}
        \end{align}
        And we have $\epsilon_{t}=\mathbf{D}^{-\frac{1}{2}}\mathbf{e}_{t}$, then
        \begin{align}
            \mathcal{L}_{sym}
                &=\mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{L}_{n \times n}\mathbf{D}_{n \times n}^{-\frac{1}{2}}\\
                &=\mathbf{D}_{n \times n}^{-\frac{1}{2}}\left(\sum_{t=1}^{m}\mathbf{e}_{t}\mathbf{e}_{t}^{\mathbf{T}}\right)\mathbf{D}_{n \times n}^{-\frac{1}{2}}\\
                &=\sum_{t=1}^{m}\left(\mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{e}_{t}\mathbf{e}_{t}^{\mathbf{T}}\mathbf{D}_{n \times n}^{-\frac{1}{2}}\right)\\
                &=\sum_{t=1}^{m}\left[\mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{e}_{t}\mathbf{e}_{t}^{\mathbf{T}}\left(\mathbf{D}_{n \times n}^{\mathbf{T}}\right)^{-\frac{1}{2}}\right]\\
                &=\sum_{t=1}^{m}\left[\mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{e}_{t}\mathbf{e}_{t}^{\mathbf{T}}\left(\mathbf{D}_{n \times n}^{-\frac{1}{2}}\right)^{\mathbf{T}}\right]\\
                &=\sum_{t=1}^{m}\left(\mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{e}_{t}\right)\left(\mathbf{D}_{n \times n}^{-\frac{1}{2}}\mathbf{e}_{t}\right)^{\mathbf{T}}\\
                &=\sum_{t=1}^{m}\epsilon_{t}\epsilon_{t}^{\mathbf{T}}
        \end{align}
    }
}

\subsection{Eigendecomposition}{
    \defn{Eigendecomposition}{
        Let $\mathbf{A}$ be a square $n \times n$ matrix with n linearly independent eigenvectors $q_{i}$ (where $i=1,\cdots,n$). Then $\mathbf{A}$ can be factored as
        \[\mathbf{A}=\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{-1}\]
        where $\mathbf{Q}$ is the square $n \times n$ matrix whose $i$-th column is the eigenvector $q_{i}$ of $\mathbf{A}$, and $\Lambda$ is the diagonal matrix whose diagonal elements are the corresponding eigenvalues, $\Lambda_{ii}=\lambda_{i}$. Note that only diagonalizable matrices can be factorized in this way.\\
        The n eigenvectors $q_{i}$ are usually normalized, but they don't have to be. A non-normalized set of n eigenvectors, $v_{i}$ can also be used as the columns of $\mathbf{Q}$.
    }

    \fact{
        Assume $\mathbf{A}$ has non-degenerate eigenvalues $\lambda_{1},lambda_{2},\cdots,\lambda_{k}$ and corresponding linearly independent eigenvectors $\mathbf{X}_{1},\mathbf{X}_{2},\cdots,\mathbf{X}_{k}$ which can be denoted
        $
            \begin{bmatrix}
                x_{11}\\
                x_{12}\\
                \vdots\\
                x_{1k}
            \end{bmatrix}
            ,
            \begin{bmatrix}
                x_{21}\\
                x_{22}\\
                \vdots\\
                x_{2k}
            \end{bmatrix}
            ,\cdots,
            \begin{bmatrix}
                x_{k1}\\
                x_{k2}\\
                \vdots\\
                x_{kk}
            \end{bmatrix}
        $.\\
        Define the matrices composed of eigenvectors
        \begin{align}
            \mathbf{Q}&=
                \begin{bmatrix}
                    \mathbf{X}_{1} & \mathbf{X}_{2} & \cdots & \mathbf{X}_{k}
                \end{bmatrix}\\
                &=
                \begin{bmatrix}
                    x_{11} & x_{21} & \cdots & x_{k1}\\
                    x_{12} & x_{22} & \cdots & x_{k2}\\
                    \vdots & \vdots & \ddots & \vdots\\
                    x_{1k} & x_{2k} & \cdots & x_{kk}
                \end{bmatrix}
        \end{align}
        and eigenvalues
        \[
            \Lambda=
                \begin{bmatrix}
                    \lambda_{1} & 0           & \cdots & 0\\
                    0           & \lambda_{2} & \cdots & 0\\
                    \vdots      & \vdots      & \ddots & \vdots\\
                    0           & 0           & \cdots & \lambda_{k}
                \end{bmatrix}
                .
        \]
        where $\Lambda$ is a diagonal matrix. Then
        \begin{align}
            \mathbf{A}\mathbf{Q}&=
                \mathbf{A}
                \begin{bmatrix}
                    \mathbf{X}_{1} & \mathbf{X}_{2} & \cdots & \mathbf{X}_{k}
                \end{bmatrix}\\
                &=
                \begin{bmatrix}
                    \mathbf{A}\mathbf{X}_{1} & \mathbf{A}\mathbf{X}_{2} & \cdots & \mathbf{A}\mathbf{X}_{k}
                \end{bmatrix}\\
                &=
                \begin{bmatrix}
                    \lambda_{1}\mathbf{X}_{1} & \lambda_{2}\mathbf{X}_{2} & \cdots & \lambda_{k}\mathbf{X}_{k}
                \end{bmatrix}\\
                &=
                \begin{bmatrix}
                    \lambda_{1}x_{11} & \lambda_{2}x_{21} & \cdots & \lambda_{k}x_{k1}\\
                    \lambda_{1}x_{12} & \lambda_{2}x_{22} & \cdots & \lambda_{k}x_{k2}\\
                    \vdots            & \vdots            & \ddots & \vdots\\
                    \lambda_{1}x_{1k} & \lambda_{2}x_{2k} & \cdots & \lambda_{k}x_{kk}
                \end{bmatrix}\\
                &=
                \begin{bmatrix}
                    x_{11} & x_{21} & \cdots & x_{k1}\\
                    x_{12} & x_{22} & \cdots & x_{k2}\\
                    \vdots & \vdots & \ddots & \vdots\\
                    x_{1k} & x_{2k} & \cdots & x_{kk}
                \end{bmatrix}
                \begin{bmatrix}
                    \lambda_{1} & 0           & \cdots & 0\\
                    0           & \lambda_{2} & \cdots & 0\\
                    \vdots      & \vdots      & \ddots & \vdots\\
                    0           & 0           & \cdots & \lambda_{k}
                \end{bmatrix}\\
                &=
                \mathbf{Q}\Lambda
        \end{align}
        giving the amazing decomposition of $\mathbf{A}$ into a similarity transformation involving $\mathbf{Q}$ and $\Lambda$,
        \[\mathbf{A}=\mathbf{Q}\Lambda\mathbf{Q}^{-1}\]
        The fact that this decomposition is always possible for a square matrix $\mathbf{A}$ as long as $\mathbf{Q}$ is a square matrix is known in this work as the Eigendecomposition theorem.
    }

    \clm{How to Perform Eigendecomposition?}{
        To perform Eigendecomposition on a matrix, follow these steps:
        \begin{enumerate}
            \item \textbf{Find the Eigenvalues}:\\
            Solve the characteristic equation:
            \[\det(\mathbf{A}-\lambda\mathbf{I})=0\]
            Here, $\mathbf{A}$ is the square matrix, $\lambda$ is the eigenvalue, and $\mathbf{I}$ is the identity matrix of the same dimension as $\mathbf{A}$.
            \item \textbf{Find the Eigenvectors}:\\
            For each eigenvalue $\lambda$, substitute it back into the equation:
            \[(\mathbf{A}-\lambda\mathbf{I})\mathbf{v}=0\]
            This represents a system of linear equations where $\mathbf{v}$ is the eigenvector corresponding to the eigenvalue $\lambda$.
            \item \textbf{Construct the Eigenvector Matrix $\mathbf{Q}$}:\\
            Place all the eigenvectors as columns in the matrix $\mathbf{Q}$. If there are n distinct eigenvalues, $\mathbf{Q}$ will be an $n \times n$ matrix.
            \item \textbf{Form the Diagonal Matrix $\Lambda$}:\\
            Construct a diagonal matrix $\Lambda$ by placing the eigenvalues on its diagonal
            \item \textbf{Calculate the Inverse of $\mathbf{Q}$}:\\
            Find $\mathbf{Q}^{-1}$, the inverse of the eigenvector matrix $\mathbf{Q}$, if the matrix is invertible.
            \item \textbf{Combine to Form the Decomposition}:\\
            Express the original matrix $\mathbf{A}$ as
            \[\mathbf{A}=\mathbf{Q}\Lambda\mathbf{Q}^{-1}\]
        \end{enumerate}
    }

    \cor{
        For general positive integer powers $n>0$,
        \[\mathbf{A}^{n}=\mathbf{Q}\Lambda^{n}\mathbf{Q}^{-1}\]
    }

    \cor{
        The inverse of $\mathbf{A}$ is
        \begin{align}
            \mathbf{A}^{-1}&=(\mathbf{Q}\Lambda\mathbf{Q}^{-1})^{-1}\\
                           &=(\mathbf{Q}^{-1})^{-1}(\Lambda)^{-1}(\mathbf{Q})^{-1}\\
                           &=\mathbf{Q}\Lambda^{-1}\mathbf{Q}^{-1}
        \end{align}
    }

    \cor{
        The inverse of the diagonal matrix $\Lambda$ is trivially given by
        \[
            \Lambda^{-1}=
                \begin{bmatrix}
                    \lambda_{1}^{-1} & 0                & \cdots & 0\\
                    0                & \lambda_{2}^{-1} & \cdots & 0\\
                    \vdots           & \vdots           & \ddots & \vdots\\
                    0                & 0                & \cdots & \lambda_{k}^{-1}
                \end{bmatrix}
        \]
    }
}

\subsection{Matrix Exponential}{
    \defn{Matrix Exponential}{
        The power series that defines the exponential map $e^{x}$ also defines a map between matrices. In particular,
        \begin{align}
            \exp(\mathbf{A})&=e^{\mathbf{A}}\\
                            &=\sum_{n=0}^{\infty}\frac{\mathbf{A}^{n}}{n!}\\
                            &=\mathbf{I}+\mathbf{A}+\frac{\mathbf{A}\mathbf{A}}{2!}+\frac{\mathbf{A}\mathbf{A}\mathbf{A}}{3!}+\cdots
        \end{align}
        converges for any square matrix $\mathbf{A}$, where $\mathbf{I}$ is the identity matrix.
    }

    \cor{
        Since $\Lambda$ is a diagonal matrix
        \[
            \Lambda=
                \begin{bmatrix}
                    \lambda_{1} & 0           & \cdots & 0\\
                    0           & \lambda_{2} & \cdots & 0\\
                    \vdots      & \vdots      & \ddots & \vdots\\
                    0           & 0           & \cdots & \lambda_{k}
                \end{bmatrix}
        \]
        Then
        \begin{align}
            \exp(\Lambda)&=e^{\Lambda}\\
                         &=\sum_{n=0}^{\infty}\frac{\Lambda^{n}}{n!}\\
                         &=\sum_{n=0}^{\infty}\frac{1}{n!}
                            \begin{bmatrix}
                                \lambda_{1}^{n} & 0               & \cdots & 0\\
                                0               & \lambda_{2}^{n} & \cdots & 0\\
                                \vdots          & \vdots          & \ddots & \vdots\\
                                0               & 0               & \cdots & \lambda_{k}^{n}
                            \end{bmatrix}\\
                         &=
                            \begin{bmatrix}
                                \sum_{n=0}^{\infty}\frac{\Lambda_{1}^{n}}{n!} & 0               & \cdots & 0\\
                                0                                             & \sum_{n=0}^{\infty}\frac{\Lambda_{2}^{n}}{n!} & \cdots & 0\\
                                \vdots                                        & \vdots                                        & \ddots & \vdots\\
                                0                                             & 0                                             & \cdots & \sum_{n=0}^{\infty}\frac{\Lambda_{k}^{n}}{n!}
                            \end{bmatrix}\\
                         &=
                            \begin{bmatrix}
                                e^\lambda_{1} & 0               & \cdots & 0\\
                                0               & e^\lambda_{2} & \cdots & 0\\
                                \vdots          & \vdots          & \ddots & \vdots\\
                                0               & 0               & \cdots & e^\lambda_{k}
                            \end{bmatrix}
        \end{align}
        Since $\mathbf{A}^{n}=\mathbf{Q}\Lambda^{n}\mathbf{Q}^{-1}$, then
        \begin{align}
            \exp(\mathbf{A})&=e^{\mathbf{A}}\\
                            &=\sum_{n=0}^{\infty}\frac{\mathbf{A}^{n}}{n!}\\
                            &=\sum_{n=0}^{\infty}\frac{\mathbf{Q}\Lambda^{n}\mathbf{Q}^{-1}}{n!}\\
                            &=\mathbf{Q}\left(\sum_{n=0}^{\infty}\frac{\mathbf{\Lambda}^{n}}{n!}\right)\mathbf{Q}^{-1}\\
                            &=\mathbf{Q}e^{\Lambda}\mathbf{Q}^{-1}
        \end{align}
        so $e^{\mathbf{A}}$ can be found using $\Lambda$.
    }

    \cor{
        For the zero matrix $\mathbf{A}=0$,
        \[
            \exp(\mathbf{A})=e^{0}=\mathbf{I}
        \]
        i.e., the identity matrix.\\
        In general,
        \[
            e^{\mathbf{A}}e^{-\mathbf{A}}=e^{0}=\mathbf{I}
        \]
        so the exponential of a matrix is always invertible, with inverse the exponential of the negative of the matrix.
    }

    \cor{
        \[\exp(\mathbf{A}^{\mathbf{T}})=\exp(\mathbf{A})^{\mathbf{T}}\]
    }

    \cor{
        If $\mathbf{Q}$ is invertible, then
        \[e^{\mathbf{Q}\mathbf{A}\mathbf{Q}^{-1}}=\mathbf{Q}e^{\mathbf{A}}\mathbf{Q}^{-1}\]
    }

    \cor{
        If $\mathbf{A}\mathbf{B}=\mathbf{B}\mathbf{A}$, then
        \[e^{\mathbf{A}}e^{\mathbf{B}}=e^{\mathbf{A}+\mathbf{B}}\]
    }

    \cor{
        \[e^{a\mathbf{A}}e^{b\mathbf{A}}=e^{(a+b)\mathbf{A}}\]
    }
}

\chapter{Classical Spectral Clustering Algorithm}

\section{Classical Spectral Clustering Algorithm}{
}

\bibliographystyle{plain}  % 参考文献样式，可选：plain, unsrt, alpha, abbrv
\bibliography{spectral_clustering_algorithms}  % 引用 references.bib 文件

\end{document}
